import java.io.*;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.Queue;
import java.util.Set;

import org.jsoup.Connection;
import org.jsoup.Jsoup;
import org.jsoup.nodes.*;
import org.jsoup.select.Elements;

public class WebCrawler {
	private String start_url;
	// REOMVED and using the database's data
	//	private static Set<String> compactStrings = new HashSet<String>();
	//	private static Set<String> visitedURLs = new HashSet<String>();
		
	private static Queue<String> linksQueue = new LinkedList<String>();
	
	private static int crawledPages = 0;
	private static MongoJava database;
	private static RobotChecker robotChecker;
	
	public WebCrawler(String start_url) {
		this.start_url = start_url;
	}
	
	public void start() {
//		String html = getDocument(start_url).html();
//		calculateCompactString(getDocument(start_url));
//		System.out.println(normalizeLink());
//		System.out.println(html);
		database = new MongoJava("mongodb://localhost:27017/","Ndry");
		robotChecker = new RobotChecker();
		try {
			crawl(start_url);
		} catch (URISyntaxException e) {
			e.printStackTrace();
		}
	}
	
	private static Document getDocument(String url) {
		try {
            // Getting connection response
			Connection.Response response = Jsoup.connect(url).userAgent("Ndry/1.0")
            		.timeout(3000)
            		.execute();
			// Checking if successful
			int status = response.statusCode();
			if(status!=200) {
				System.out.println("Skipped NON-OK response from url" + url);
				return null;
			}
			
			if(response.contentType()==null || !response.contentType().toLowerCase().contains("text/html")) {
				System.out.println("Skipped NON-HTML content from " + url);
				return null;
			}
			
			
            // return the HTML document
            return response.parse(); 
        } catch (IOException e) {
            // handle exceptions
            System.err.println("Unable to fetch HTML of: " + url);
        }
        return null;
    }
	
	private static String calculateCompactString(Document doc) {
		try {
			
			Elements elements = doc.body().select("*");	// selects all content
			
			String[] list = elements.text().split("\\s+");	// Splitting by One or More Whitespace Characters
			String cs = "";
			for(String word:list) {
				word = word.trim();
				if(word.length()>2)
					cs+=word.charAt(0);
			}

			return cs;
			
		}catch(Exception e) {
			e.printStackTrace();
			return null;
		}
	}
	
	private static String normalizeLink(String url) throws URISyntaxException {
		URI uri = new URI(url);
		uri = new URI(uri.getScheme(), uri.getAuthority(), uri.getPath(), null, null);	// normalizing it
		String normalizedLink = uri.toString();
		return normalizedLink;
	}
	
	private static void crawl(String url) throws URISyntaxException {
		if (!url.startsWith("http://") && !url.startsWith("https://")) {
			System.err.println("Skipping invalid URL: " + url);
            return;
		}
		linksQueue.add(url);
		database.enqueueUrl(url);
		while(!linksQueue.isEmpty() && crawledPages<5) {
			
			url = linksQueue.remove();
			database.dequeueUrl();
			url = normalizeLink(url);

			if(!RobotChecker.isUrlAllowed(url)) {
				System.out.println("NOT ALLOWED");
				continue;
			}

			if(database.isVisited(url)) {// <-- changed after database
				continue;
			}
			
			Document doc = getDocument(url);
			if(doc == null) {
				continue;
			}
			String cs = calculateCompactString(doc);
			
			if(database.hasCompactString(cs)) {
				continue;
			}
			
			// adding the compact string to database
//			compactStrings.add(cs); <-- removed after database
			database.addCompactString(cs);
			
			System.out.println(crawledPages);
			
			// log current URL
	        System.out.println("Crawling: " + url);
	        
//	        visitedURLs.add(url); <-- removed after database
	        // Queuing the link
	        database.enqueueUrl(url);
	        // Mark it as visited
	        database.markVisited(url);
	        
       	 	crawledPages++;
	        Elements links = doc.select("a[href]");
	        for(Element link:links) {
	        	 String nextUrl = link.absUrl("href");
	             // check if nextUrl exists and link hasn't been visited 
	             if (!nextUrl.isEmpty() && !database.isVisited(nextUrl)) {
//	                 crawl(nextUrl); 
	            	 linksQueue.add(nextUrl);
//	            	 System.out.println(nextUrl);
	             }
	        }
	        
		}
		System.out.println("finished crawling");
	}

}
