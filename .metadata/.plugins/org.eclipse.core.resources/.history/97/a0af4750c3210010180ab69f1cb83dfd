import crawlercommons.robots.BaseRobotsParser;
import crawlercommons.robots.SimpleRobotRules;
import crawlercommons.robots.SimpleRobotRulesParser;
// Removed crawlercommons.fetcher imports

import org.jsoup.Connection; // Added Jsoup import
import org.jsoup.HttpStatusException; // Added Jsoup import
import org.jsoup.Jsoup; // Added Jsoup import

import java.io.IOException; // Added IO import
import java.net.MalformedURLException; // Added URL import
import java.net.URL;
import java.nio.charset.StandardCharsets;
import java.util.HashMap; // Added Map import
import java.util.Map;

public class RobotChecker {

    // Define YOUR crawler's user agent string - Be specific and identifiable!
    private static final String MY_USER_AGENT = "MyCoolBot/1.0 (+http://mywebsite.com/botinfo)";

    // Cache for robots rules per host to avoid re-fetching constantly
    private static final Map<String, SimpleRobotRules> robotsCache = new HashMap<>();
    private static final long CACHE_DURATION_MS = 6 * 60 * 60 * 1000; // e.g., 6 hours
    private static final Map<String, Long> cacheTimestamps = new HashMap<>();


    /**
     * Checks if crawling the given URL is allowed according to robots.txt.
     * Fetches and parses robots.txt if not cached or expired.
     *
     * @param urlToCheck The full URL string to check.
     * @return true if allowed, false otherwise.
     */
    public static boolean isUrlAllowed(String urlToCheck) {
        try {
            URL url = new URL(urlToCheck);
            String host = url.getHost().toLowerCase();
            String protocol = url.getProtocol();
            int port = url.getPort() != -1 ? url.getPort() : url.getDefaultPort();
            String hostKey = protocol + "://" + host + ":" + port; // Cache key includes protocol and port

            SimpleRobotRules rules = getRobotRules(hostKey);

            // crawler-commons returns EMPTY_RULES if fetch fails or no rules apply
            // It doesn't typically return null like the previous example assumed
            if (rules == null) { // Should ideally not happen with crawler-commons logic
                 System.err.println("Unexpected null rules for " + hostKey + ", disallowing for safety.");
                 return false; // Be cautious
            }

            // Check allowance using the rules object
            boolean allowed = rules.isAllowed(urlToCheck);
            System.out.println("Robots check (crawler-commons) for " + urlToCheck + ": " + (allowed ? "Allowed" : "Disallowed"));
            return allowed;

        } catch (MalformedURLException e) {
            System.err.println("Invalid URL format: " + urlToCheck);
            return false; // Cannot check an invalid URL
        } catch (Exception e) {
            System.err.println("Unexpected error checking robots with crawler-commons for " + urlToCheck + ": " + e.getMessage());
            // Log e for more details if needed
            // e.printStackTrace();
            return false; // Be cautious on unexpected errors
        }
    }

    /**
     * Gets the SimpleRobotRules for a host, using cache or fetching/parsing new rules.
     */
    private static SimpleRobotRules getRobotRules(String hostKey) {
        long now = System.currentTimeMillis();
        // Check cache first
        if (robotsCache.containsKey(hostKey) && (now - cacheTimestamps.getOrDefault(hostKey, 0L)) < CACHE_DURATION_MS) {
             System.out.println("Using cached robots.txt rules for: " + hostKey);
             return robotsCache.get(hostKey);
        }

        System.out.println("Fetching robots.txt for: " + hostKey);
        String robotsUrl = hostKey + "/robots.txt";
        // *** Use the Jsoup fetch method ***
        byte[] robotsBytes = fetchRobotsTxtBytesWithJsoup(robotsUrl);

        SimpleRobotRules rules;
        if (robotsBytes != null) {
            // Use the crawler-commons parser
            BaseRobotsParser parser = new SimpleRobotRulesParser();
            // Provide fetched URL, content bytes, expected content type, and YOUR user agent
            // Guesses encoding, but UTF-8 is common. Might need refinement if issues arise.
            rules = (SimpleRobotRules) parser.parseContent(robotsUrl, robotsBytes, "text/plain", MY_USER_AGENT);
            System.out.println("Parsed robots.txt for " + hostKey);
        } else {
            // If robots.txt doesn't exist or fetch failed, create rules that allow everything
            rules = new SimpleRobotRules(SimpleRobotRules.RobotRulesMode.ALLOW_ALL);
            System.out.println("robots.txt not found or fetch failed for " + hostKey + ", allowing all.");
        }

        // Store the result in cache
        robotsCache.put(hostKey, rules);
        cacheTimestamps.put(hostKey, now);

        return rules;
    }


    /**
     * Fetches the content of robots.txt using Jsoup.
     *
     * @param robotsUrl The full URL to the robots.txt file.
     * @return Byte array of the content, or null if fetch fails significantly (e.g., non-2xx/404 status, network error).
     *         Returns an empty byte array for 404, signifying "allow all".
     */
    private static byte[] fetchRobotsTxtBytesWithJsoup(String robotsUrl) {
        try {
            Connection.Response response = Jsoup.connect(robotsUrl)
                    .userAgent(MY_USER_AGENT) // Use same user agent for fetching robots.txt
                    .timeout(5000) // Reasonable timeout
                    .ignoreContentType(true) // We want the raw bytes regardless of declared type
                    .followRedirects(true)
                    .execute();

            // Check for success (2xx)
            if (response.statusCode() >= 200 && response.statusCode() < 300) {
                return response.bodyAsBytes();
            } else if (response.statusCode() == 404 || response.statusCode() == 410 || response.statusCode() == 403 || response.statusCode() == 401) {
                // Treat common "access denied" or "not found" as allow all by returning empty bytes
                // The parser interprets empty content as allow all.
                System.out.println("robots.txt returned " + response.statusCode() + " for " + robotsUrl + " -> allowing all.");
                // Return empty array instead of null to distinguish from network errors
                return new byte[0];
            } else {
                 // Other errors (5xx etc.) - might be temporary, maybe disallow?
                 System.err.println("Failed to fetch robots.txt from " + robotsUrl + " - Status: " + response.statusCode() + ". Disallowing for safety.");
                // Returning null here might be treated as "allow" by getRobotRules - maybe return specific error indicator?
                // For now, returning null, getRobotRules will create ALLOW_ALL rules based on current logic.
                return null;
            }
        } catch (HttpStatusException e) {
             // Catch specific HTTP status errors Jsoup might throw
             if (e.getStatusCode() == 404 || e.getStatusCode() == 410 || e.getStatusCode() == 403 || e.getStatusCode() == 401) {
                 System.out.println("robots.txt returned " + e.getStatusCode() + " (HttpStatusException) for " + robotsUrl + " -> allowing all.");
                 return new byte[0]; // Treat as allow all
             } else {
                 System.err.println("HTTP error fetching robots.txt from " + robotsUrl + ": " + e.getStatusCode() + " " + e.getMessage() + ". Disallowing for safety.");
                 return null;
             }
        } catch (IOException e) {
            // Network errors, timeouts, etc.
            System.err.println("Network error fetching robots.txt from " + robotsUrl + ": " + e.getMessage() + ". Disallowing for safety.");
            return null;
        } catch (Exception e) {
             // Catch-all for unexpected errors during fetch
             System.err.println("Unexpected error fetching robots.txt from " + robotsUrl + ": " + e.getMessage() + ". Disallowing for safety.");
             e.printStackTrace(); // Log stack trace for unexpected errors
             return null;
        }
    }

    // --- Example Usage ---
    public static void main(String[] args) throws InterruptedException {
        System.out.println("Checking Google Search:");
        isUrlAllowed("https://www.google.com/search?q=test"); // Likely disallowed

        System.out.println("\nChecking Google Allowed:");
        isUrlAllowed("https://www.google.com/something/allowed"); // Should be allowed by default /robots.txt

        System.out.println("\nChecking Wikipedia Main Page:");
        isUrlAllowed("https://en.wikipedia.org/wiki/Main_Page"); // Usually allowed

        System.out.println("\nChecking Wikipedia Special Page:");
        isUrlAllowed("https://en.wikipedia.org/wiki/Special:Random"); // Often disallowed

        System.out.println("\nChecking Non-existent Robots:");
        isUrlAllowed("https://www.example.com/somepage"); // Should default to allowed

        // Test caching
        System.out.println("\nRe-Checking Wikipedia Main Page (should use cache):");
        isUrlAllowed("https://en.wikipedia.org/wiki/Main_Page");

        // Test 404
        System.out.println("\nChecking site likely without robots.txt (e.g., a local test):");
        // Replace with a URL you know gives 404 for robots.txt if possible
        isUrlAllowed("http://localhost:9999/test"); // Assuming nothing runs here

    }
}