import java.io.*;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.Queue;
import java.util.Set;

import org.jsoup.Connection;
import org.jsoup.Jsoup;
import org.jsoup.nodes.*;
import org.jsoup.select.Elements;

public class WebCrawler {
	private String start_url;
	private static Set<String> compactStrings = new HashSet<String>();
	private static Set<String> visitedURLs = new HashSet<String>();
	
	private static Queue<String> linksQueue = new LinkedList<String>();
	
	private static int crawledPages = 0;
	
	public WebCrawler(String start_url) {
		this.start_url = start_url;
	}
	
	public void start() {
		String html = getDocument(start_url).html();
		
//		calculateCompactString(getDocument(start_url));
//		System.out.println(normalizeLink());
		System.out.println(html);
//		
//		try {
//			crawl(start_url);
//		} catch (URISyntaxException e) {
//			e.printStackTrace();
//		}
	}
	
	private static Document getDocument(String url) {
		try {
            // Getting connection response
			Connection.Response response = Jsoup.connect(url).userAgent("Ndry/1.0")
            		.timeout(3000)
            		.execute();
			// Checking if successful
			int status = response.statusCode();
			if(status!=200) {
				System.out.println("Skipped NON-OK response from url" + url);
				return null;
			}
			
			if(response.contentType()==null || !response.contentType().toLowerCase().contains("text/html")) {
				System.out.println("Skipped NON-HTML content from " + url);
				return null;
			}
			
			
            // return the HTML document
            return response.parse(); 
        } catch (IOException e) {
            // handle exceptions
            System.err.println("Unable to fetch HTML of: " + url);
        }
        return null;
    }
	
	private static String calculateCompactString(Document doc) {
		try {
			
			Elements elements = doc.body().select("*");	// selects all content
			
			String[] list = elements.text().split("\\s+");	// Splitting by One or More Whitespace Characters
			String cs = "";
			for(String word:list) {
				word = word.trim();
				if(word.length()>2)
					cs+=word.charAt(0);
			}

			return cs;
			
		}catch(Exception e) {
			e.printStackTrace();
			return null;
		}
	}
	
	private static String normalizeLink(String url) throws URISyntaxException {
		URI uri = new URI(url);
		uri = new URI(uri.getScheme(), uri.getAuthority(), uri.getPath(), null, null);	// normalizing it
		String normalizedLink = uri.toString();
		return normalizedLink;
	}
	
	private static void crawl(String url) throws URISyntaxException {
		if (!url.startsWith("http://") && !url.startsWith("https://")) {
			System.err.println("Skipping invalid URL: " + url);
            return;
		}
		linksQueue.add(url);
		while(crawledPages<5) {
			url = linksQueue.remove();
			url = normalizeLink(url);
			if(visitedURLs.contains(url)) {
				continue;
			}
			Document doc = getDocument(url);
			if(doc == null) {
				continue;
			}
			String cs = calculateCompactString(doc);
			
			if(compactStrings.contains(cs)) {
				continue;
			}
			
			// adding the compact string to hash set
			compactStrings.add(cs);
			
			System.out.println(crawledPages);
			
			// log current URL
	        System.out.println("Crawling: " + url);
	        
	        visitedURLs.add(url);
       	 	crawledPages++;
	        Elements links = doc.select("a[href]");
	        for(Element link:links) {
	        	 String nextUrl = link.absUrl("href");
	             // check if nextUrl exists and link hasn't been visited 
	             if (!nextUrl.isEmpty() && !visitedURLs.contains(nextUrl)) {
//	                 crawl(nextUrl); 
	            	 linksQueue.add(nextUrl);
//	            	 System.out.println(nextUrl);
	             }
	        }
	        
		}
		System.out.println("finished crawling");
	}

}
