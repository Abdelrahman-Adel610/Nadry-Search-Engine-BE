import crawlercommons.robots.BaseRobotsParser;
import crawlercommons.robots.SimpleRobotRules;
import crawlercommons.robots.SimpleRobotRulesParser;
import crawlercommons.fetcher.http.SimpleHttpFetcher; // Or use Jsoup/other to fetch bytes
import crawlercommons.fetcher.Payload; // If using SimpleHttpFetcher

import java.net.URL;
import java.nio.charset.StandardCharsets;
// ... other imports

public class RobotsCheckerCrawlerCommons {

    private static final String MY_USER_AGENT = "MyCoolBot/1.0 (+http://mywebsite.com/botinfo)";
    // Cache map would store SimpleRobotRules instead of RobotsMatcher
    private static Map<String, SimpleRobotRules> robotsCache = new HashMap<>();
    // ... caching logic similar to before ...

    public static boolean isUrlAllowed(String urlToCheck) {
        try {
            URL url = new URL(urlToCheck);
            String host = url.getHost().toLowerCase();
            String protocol = url.getProtocol();
            int port = url.getPort() != -1 ? url.getPort() : url.getDefaultPort();
            String hostKey = protocol + "://" + host + ":" + port;

            SimpleRobotRules rules = getRobotRules(hostKey);

            // crawler-commons returns EMPTY_RULES if fetch fails or no rules apply
            // It doesn't typically return null like the previous example assumed
            if (rules == null) { // Should ideally not happen with crawler-commons logic
                 System.err.println("Unexpected null rules for " + hostKey);
                 return false; // Be cautious
            }

            // Check allowance using the rules object
            boolean allowed = rules.isAllowed(urlToCheck);
            System.out.println("Robots check (crawler-commons) for " + urlToCheck + ": " + (allowed ? "Allowed" : "Disallowed"));
            return allowed;

        } catch (Exception e) {
            System.err.println("Error checking robots with crawler-commons for " + urlToCheck + ": " + e.getMessage());
            return false;
        }
    }

    private static SimpleRobotRules getRobotRules(String hostKey) {
         // --- Caching logic here (check timestamp, return from cache if valid) ---
         // ... similar to previous example ... if cached, return robotsCache.get(hostKey);

         System.out.println("Fetching robots.txt for: " + hostKey);
         String robotsUrl = hostKey + "/robots.txt";
         byte[] robotsBytes = fetchRobotsTxtBytes(robotsUrl); // Use your fetch method

         SimpleRobotRules rules;
         if (robotsBytes != null) {
             // Use the crawler-commons parser
             BaseRobotsParser parser = new SimpleRobotRulesParser();
             // Provide fetched URL, content bytes, expected content type, and YOUR user agent
             rules = parser.parseContent(robotsUrl, robotsBytes, "text/plain", MY_USER_AGENT);
         } else {
             // If robots.txt doesn't exist or fetch failed, create rules that allow everything
             rules = new SimpleRobotRules(SimpleRobotRules.RobotRulesMode.ALLOW_ALL);
             System.out.println("robots.txt not found or fetch failed for " + hostKey + ", allowing all.");
         }

        // --- Store 'rules' in robotsCache with timestamp ---
        // ... robotsCache.put(hostKey, rules); cacheTimestamps.put(hostKey, System.currentTimeMillis()); ...

        return rules;
    }

     // Your method to fetch bytes - can use Jsoup as before
     private static byte[] fetchRobotsTxtBytes(String robotsUrl) {
          // ... implement using Jsoup.connect().execute().bodyAsBytes() ...
          // ... handle 404 (return null), handle other errors (return null) ...
          return null; // Placeholder
     }
     // ... main method for testing ...
}