package webCrawler;

import java.io.IOException;
import java.util.LinkedList;
import java.util.List; // Import List
import java.util.Scanner;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.nio.file.Path; // Import Path

public class WebCrawlerMain {

    private final static int MAX_CRAWL_LIMIT = 100; // Define limit here
    private final static int DEFAULT_THREADS = 4;

    public static void main(String[] args) {
        // --- Configuration ---
        List<String> initialSeedUrls = new LinkedList<>(); // Use List interface
        String dbConnectionString = "mongodb://localhost:27017/";
        String dbName = "Ndry"; // Database name
        int numberOfThreads = DEFAULT_THREADS;

        // --- Database Initialization ---
        MongoJava database = null;
        ExecutorService executor = null; // Declare here for finally block

        try {
            database = new MongoJava(dbConnectionString, dbName);
            int pageCount = database.getCrawledCount(); // Get initial count from DB

            // --- Seed Logic ---
            // Decide whether to use fresh seeds or rely on existing queue
            boolean startWithFreshSeeds = false; // Set to true to always clear queue and add seeds
            if (startWithFreshSeeds || pageCount == -1 /* Error reading count? */ || database.isQueueEmpty() /* Start if queue empty? */ ) {
                 // Optional: Clear existing queue if starting fresh?
                 // database.clearQueue(); // You would need to implement this method in MongoJava

                System.out.println("Initializing with fresh seeds.");
                // Add your seed URLs
                initialSeedUrls.add("https://en.wikipedia.org/wiki/Main_Page");
                initialSeedUrls.add("https://news.google.com/");
                initialSeedUrls.add("https://jsoup.org/");
                // Add more diverse seeds...

                // Enqueue initial seeds (make sure they are normalized)
                for (String seed : initialSeedUrls) {
                    try {
                        String normalizedSeed = WebCrawler.normalizeLink(seed); // Use static method
                         if (!database.isVisited(normalizedSeed)) {
                              database.enqueueUrl(normalizedSeed);
                         }
                    } catch (Exception e) {
                        System.err.println("Skipping invalid seed URL: " + seed + " - " + e.getMessage());
                    }
                }
            } else {
                System.out.println("Resuming crawl. Initial page count: " + pageCount + ". Queue has items.");
            }

            // --- Get Thread Count from User ---
            Scanner sc = new Scanner(System.in);
            System.out.print("Enter the number of threads [" + DEFAULT_THREADS + "]: ");
            try {
                String input = sc.nextLine();
                if (!input.trim().isEmpty()) {
                    numberOfThreads = Integer.parseInt(input);
                    if (numberOfThreads <= 0) {
                        System.err.println("Warning: Number of threads must be positive. Using default: " + DEFAULT_THREADS);
                        numberOfThreads = DEFAULT_THREADS;
                    }
                } else {
                     System.out.println("Using default thread count: " + DEFAULT_THREADS);
                }
            } catch (NumberFormatException e) {
                System.err.println("Warning: Invalid number format. Using default: " + DEFAULT_THREADS);
                numberOfThreads = DEFAULT_THREADS;
            } finally {
                sc.close(); // Close scanner
            }


            // --- Setup and Start Thread Pool ---
            System.out.println("Starting crawler with " + numberOfThreads + " threads. Max pages: " + MAX_CRAWL_LIMIT);
            executor = Executors.newFixedThreadPool(numberOfThreads);

            // Create and submit worker tasks
            for (int i = 0; i < numberOfThreads; i++) {
                // Pass the single shared database instance to each worker
                executor.submit(new WebCrawler(database)); // Simplified constructor
            }

            // --- Wait for Completion (Improved Logic Needed) ---
            // This section needs a more robust way to detect completion.
            // Options:
            // 1. Periodically check if queue is empty AND no threads are actively processing.
            // 2. Use a Phaser or CountDownLatch coordinated by workers.
            // 3. Simply let it run and rely on manual interruption (Ctrl+C) or the page limit signal.

            System.out.println("Crawler workers running. Monitoring or waiting for limit/interruption...");

            // Example: Monitor queue size and potentially signal shutdown (basic)
            while (!executor.isTerminated()) {
                try {
                    Thread.sleep(10000); // Check every 10 seconds
                    // A more robust check would involve seeing if the queue
                    // has been empty for a while AND if threads are idle (waiting).
                    // This is complex to get right.
                    if (database.isQueueEmpty()) {
                         // Maybe wait a bit longer to ensure threads aren't just about to add more
                         Thread.sleep(5000);
                         if (database.isQueueEmpty()) {
                             System.out.println("Queue appears empty, signaling shutdown...");
                             WebCrawler.signalShutdown(); // Signal threads to stop
                         }
                    }
                    // Check if page limit was reached via the signal
                    if (WebCrawler.shutdownSignal.get()) { // Access static signal
                        System.out.println("Shutdown signal detected by main thread.");
                        break; // Exit monitoring loop
                    }
                } catch (InterruptedException e) {
                    System.out.println("Main thread interrupted, signaling shutdown...");
                    WebCrawler.signalShutdown(); // Signal threads on interrupt
                    Thread.currentThread().interrupt(); // Preserve status
                    break;
                }
            }


        } catch (Exception e) { // Catch exceptions during setup
            System.err.println("Critical error setting up crawler:");
            e.printStackTrace();
        } finally {
            // --- Shutdown Executor ---
            System.out.println("Initiating shutdown sequence...");
            if (executor != null && !executor.isShutdown()) {
                WebCrawler.signalShutdown(); // Ensure signal is set
                try {
                    System.out.println("Shutting down executor pool...");
                    executor.shutdown(); // Allow submitted tasks to finish
                    if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                        System.err.println("Pool did not terminate after 60s, forcing shutdown...");
                        executor.shutdownNow();
                        if (!executor.awaitTermination(60, TimeUnit.SECONDS))
                            System.err.println("Pool did not terminate.");
                    } else {
                        System.out.println("Executor pool terminated successfully.");
                    }
                } catch (InterruptedException ie) {
                    executor.shutdownNow();
                    Thread.currentThread().interrupt();
                }
            }

            // --- Close Database ---
            if (database != null) {
                database.close();
            }

            // --- Print Results ---
            System.out.println("\n--- Crawl Finished ---");
            // Access static lists AFTER threads have joined/terminated
            System.out.println("Total pages attempted (approx): " + database.getCrawledCount()); // Get final count
            System.out.println("Number of unique documents saved: " + WebCrawler.links.size());
            System.out.println("Saved Documents Paths & Links:");
            List<String> finalLinks = WebCrawler.links; // Use synchronized list directly
            List<Path> finalPaths = WebCrawler.paths;
            for (int i = 0; i < finalLinks.size(); i++) {
                 System.out.println("  " + finalLinks.get(i) + " -> " + (i < finalPaths.size() ? finalPaths.get(i) : "Path missing?"));
            }
        }
    }
}